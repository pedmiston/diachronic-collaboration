---
title: "The success of iteration and teamwork as strategies for winning Kaggle competitions"
author: "Pierce Edmiston"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
---

Kaggle competitions are data-oriented challenges in which teams and individuals use statistics and machine learning to make accurate predictions about unlabeled data. Teams can use any method to generate the predictions. Some methods work better for some problems than others, but what strategies work for all problems? This report investigates the success of two possible strategies to completing Kaggle competitions: **iteration** and **team size**. Iteration as a strategy in a Kaggle competition is making multiple submissions with the goal of incrementally improving performance each time. Do teams that make more submissions have a better chance of winning? The second strategy investigated in this report is the effect of team size. How much do additional teammates improve a team's chance of success?

```{r config, echo = FALSE}
library(knitr)
library(broom)
library(pander)

opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  dev = c("svg", "png"),
  fig.path = "figs/",
  fig.height = 3.5,
  fig.width = 4,
  cache = TRUE,
  cache.path = ".cache/"
)

read_chunk("chunks/kaggle.R")
```

```{r setup}
```

# What is Kaggle?

[Kaggle competitions](https://kaggle.com) are data-oriented challenges in which teams and individuals use statistics and machine learning to make accurate predictions about unlabeled data. For example, a standard Kaggle competition is to predict the survivors of the Titanic. Given a passenger's ticket information, how accurately can teams predict whether or not the passenger survived? This example is a bit more morbid than usual, but otherwise it's illustrative of the fact that perfect performance is rarely possible in a Kaggle competition. It's likely impossible to predict exactly who survived the Titanic based only on ticket information, but people can do far better than chance. The way teams improve their predictions is by using statistics and machine learning to generate more and more accurate predictions. Teams can use any method in statistics and machine learning to generate the predictions, which means there are many possible solutions to any Kaggle competition. The goal of this report is to determine whether iteration and team size are successful strategies to approaching Kaggle competitions regardless of what specific method is being used.

# Iteration

Iteration as a strategy for solving a Kaggle competition is exhibited by teams making many submissions, each incrementally improving upon the previous ones. In this section I explore whether this is a worthwhile strategy for competing in a data science competition. Do teams that make more submissions tend to do better than teams who make fewer submissions? Any answer to this question that can be obtained through Kaggle competition leaderboards is correlational in nature, and cannot address the causal impact of imposing iteration as an experimental manipulation in how teams interact. However, there are a number of things that **should be true** if it were the case that iteration is a successful strategy in Kaggle competitions. First, we should expect to see that better placing teams tend to make more submissions. 

```{r submissions-from-place}
```

The analysis above averages by place across competition. A different way to plot the same data is as number of submissions relative to the number of submissions made by the first place team. This analysis is specific to each competition and better reflects the extent to which first place teams make more submissions than other teams in the same competition.

```{r relative-submissions-from-place}
```

Although this analysis makes sense as a correlation, it does not lend itself to causal predictions. For example, from this analysis alone we do not know the extent to which making more submissions is actually associated with an improvement in place. A more accurate depiction of this relationship predicts place by the number of submissions made by a particular team. Although the distribution of submission numbers is much more skewed than the distribution of final places in a competition, there still is a positive relationship such that teams that make more submissions tend to place better in the competition.

```{r place-from-submissions}
```

However, even this analysis is an inadequate test of the primary hypothesis because it does not reflect the ability of each team incrementally improving their solution over iterative solutions. If iteration is associated with an improvement in solution success, then as teams generate more and more solutions they should improve in place each time.

To do this analysis across competitions with different distributions of scores, I needed to assign each (pre-final) submission a predicted place based on how that score would have placed against the final leaderboard.

```{r predicted-place-from-submission}
```

Kaggle competitors that make more submissions tend to place higher in the final standings in part because solutions tend to improve iteratively over submissions.

# Team size

```{r teamsize-from-place}
```

```{r place-from-teamsize}
```

```{r place-from-teamsize-zoom}
```

# Correlation

Do teams that are bigger tend to make more submissions?

```{r correlation}
```
